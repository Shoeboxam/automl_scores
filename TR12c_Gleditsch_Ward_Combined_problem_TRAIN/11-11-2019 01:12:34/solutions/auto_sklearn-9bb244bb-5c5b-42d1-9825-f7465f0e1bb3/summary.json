{"model": "AutoSklearnClassifier", "description": "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n           delete_tmp_folder_after_terminate=True,\n           disable_evaluator_output=False, ensemble_memory_limit=1024,\n           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n           exclude_preprocessors=None, get_smac_object_callback=None,\n           include_estimators=None, include_preprocessors=None,\n           initial_configurations_via_metalearning=25, logging_config=None,\n           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n           output_folder=None, per_run_time_limit=360,\n           resampling_strategy='cv',\n           resampling_strategy_arguments={'shuffle': False, 'folds': 10},\n           seed=1, shared_mode=False, smac_scenario_args=None,\n           time_left_for_this_task=600, tmp_folder=None)", "model_id": "9bb244bb-5c5b-42d1-9825-f7465f0e1bb3", "search_id": "b0d13faf-2cb6-477b-be60-284041c075c3", "system": "auto_sklearn", "scores": [{"value": 0.5, "metric": {"metric": "ROC_AUC"}, "target": "mido"}, {"value": 0.9901852261825695, "metric": {"metric": "ACCURACY"}, "target": "mido"}, {"value": 0, "metric": {"metric": "PRECISION"}, "target": "mido"}, {"value": 0, "metric": {"metric": "RECALL"}, "target": "mido"}, {"value": 0, "metric": {"metric": "F1"}, "target": "mido"}], "produce": [{"input": {"name": "all", "resource_uri": "file:///ravens_volume/test_data/TR12c_Gleditsch_Ward_Combined/TRAIN/dataset_TRAIN/tables/learningData.csv"}, "configuration": {"predict_type": "RAW"}, "data_pointer": "/ravens_volume/solvers/produce/79989c17-1a3a-4584-8543-6f34e04b44c5.csv"}, {"input": {"name": "all", "resource_uri": "file:///ravens_volume/test_data/TR12c_Gleditsch_Ward_Combined/TRAIN/dataset_TRAIN/tables/learningData.csv"}, "configuration": {"predict_type": "PROBABILITIES"}, "data_pointer": "/ravens_volume/solvers/produce/0dcbbd19-d272-47b7-b1aa-088dc2cc1475.csv"}, {"input": {"name": "test", "resource_uri": "file:///ravens_volume/test_output/TR12c_Gleditsch_Ward_Combined/additional_inputs/test/ws_505/2019-11-11_00-26-13/tables/learningData.csv"}, "configuration": {"predict_type": "RAW"}, "data_pointer": "/ravens_volume/solvers/produce/78d4ff8e-de05-41dd-8ce3-745cdb5f8561.csv"}, {"input": {"name": "test", "resource_uri": "file:///ravens_volume/test_output/TR12c_Gleditsch_Ward_Combined/additional_inputs/test/ws_505/2019-11-11_00-26-13/tables/learningData.csv"}, "configuration": {"predict_type": "PROBABILITIES"}, "data_pointer": "/ravens_volume/solvers/produce/fe211367-9194-404d-9e67-ac0d37004453.csv"}, {"input": {"name": "train", "resource_uri": "file:///ravens_volume/test_output/TR12c_Gleditsch_Ward_Combined/additional_inputs/train/ws_505/2019-11-11_00-26-13/tables/learningData.csv"}, "configuration": {"predict_type": "RAW"}, "data_pointer": "/ravens_volume/solvers/produce/309f6235-8513-49ed-91a0-c8fe56165608.csv"}, {"input": {"name": "train", "resource_uri": "file:///ravens_volume/test_output/TR12c_Gleditsch_Ward_Combined/additional_inputs/train/ws_505/2019-11-11_00-26-13/tables/learningData.csv"}, "configuration": {"predict_type": "PROBABILITIES"}, "data_pointer": "/ravens_volume/solvers/produce/a5cea462-4dae-4177-84ca-8d0e1631fe4d.csv"}, {"input": {"name": "partials", "resource_uri": "file:///ravens_volume/test_output/TR12c_Gleditsch_Ward_Combined/additional_inputs/partials/ws_505/2019-11-11_00-26-12/tables/learningData.csv"}, "configuration": {"predict_type": "RAW"}, "data_pointer": "/ravens_volume/solvers/produce/4e2e8229-233c-4a57-b4a3-1556338977cb.csv"}, {"input": {"name": "partials", "resource_uri": "file:///ravens_volume/test_output/TR12c_Gleditsch_Ward_Combined/additional_inputs/partials/ws_505/2019-11-11_00-26-12/tables/learningData.csv"}, "configuration": {"predict_type": "PROBABILITIES"}, "data_pointer": "/ravens_volume/solvers/produce/a4c6c6d3-6782-4d17-a9ea-4454f99642b1.csv"}]}